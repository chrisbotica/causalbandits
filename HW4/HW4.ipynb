{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW 4\n",
    "## Chris Botica and Nick Tyler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Necessity and Sufficiency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Probability of Neccessity and Sufficiency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we assume monotonicity, by **Theorem 9.2.15** (pg 293) of Causality, we have:\n",
    "\n",
    "\\begin{align} PNS = P(Y=1|do(X=1))-P(Y=1|do(X=0)) \\end{align}\n",
    "\n",
    "\\begin{align} PN = \\frac{P(Y=1)-P(Y=1|do(X=0))}{P(Y=1,X=1)} \\end{align}\n",
    "\n",
    "\\begin{align} PS = \\frac{P(Y=1|do(X=1))-P(Y=1)}{P(Y=0,X=0)} \\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the theorem above we have that \n",
    "\n",
    "\\begin{align} PN = \\frac{P(Y=1)-P(Y=1|do(X=0))}{P(Y=1,X=1)} \\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the SCM we have that $y=(x \\wedge q)\\vee n_y$. Since $x=n_x$ and $q=n_q$, this is mathematically equivalent to       $y=(n_x \\wedge n_q)\\vee n_y$. Now, since $n_x \\sim BernoulliBool(p=0.5)$, $n_y \\sim BernoulliBool(p=0.9)$, and  $n_q  \\sim BernoulliBool(p=0.2)$, we conclude that $P(n_x=1)=0.5$, $P(n_q=1)=0.9$, and $P(n_y=1)=0.2$.\n",
    "\n",
    "Thus, since $n_x$, $n_q$, $n_y$ are independent, we have\n",
    "\\begin{align}  P(Y=1) &= P((n_x \\wedge n_q)\\vee n_y) \\\\\n",
    "                      &= P(n_x \\wedge n_q)+P(n_y)-P(n_x \\wedge n_q \\wedge n_y) \\\\\n",
    "                      &= P(n_x)P(n_q)+P(n_y)-P(n_x)P(n_q)P(n_y) \\\\\n",
    "                      &= 0.5*0.9+0.2-0.5*0.9*0.2 \\\\\n",
    "                      &= 0.56 \\end{align}\n",
    "\n",
    "\n",
    "                      \n",
    "Using the SCM we get\n",
    "\\begin{align}  P(Y=1|do(X=0)) &= P((0 \\wedge q)\\vee n_y) \\\\\n",
    "                              &= P((0 \\wedge n_q)\\vee n_y) \\;\\;\\; \\text{(since $q=n_q$)}\\\\\n",
    "                              &= P(0 \\vee n_y) \\\\   \n",
    "                              &= P(n_y) \\\\ \n",
    "                              &= 0.2 \\end{align}\n",
    "\n",
    "And finally,\n",
    "\\begin{align}  P(Y=1,X=1) &= P((1 \\wedge n_q)\\vee n_y) \\\\\n",
    "                              &= P(n_q \\vee n_y) \\\\   \n",
    "                              &= P(n_q)+P(n_y)-P(n_q \\wedge n_q) \\\\ \n",
    "                              &= 0.9+0.2-0.9*0.2 \\\\ \n",
    "                              &= 0.92 \\end{align}\n",
    "                              \n",
    "Therefore, we have\n",
    "\\begin{align} PN &= \\frac{P(Y=1)-P(Y=1|do(X=0))}{P(Y=1,X=1)} \\\\\n",
    "                 &= \\frac{0.56-0.2}{0.92} \\\\\n",
    "                 &= 0.39 \\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the theorem above we also have \n",
    "\n",
    "\\begin{align} PS = \\frac{P(Y=1|do(X=1))-P(Y=1)}{P(Y=0,X=0)} \\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the SCM, and the fact that $n_x$, $n_q$, $n_y$ are independent, we have:\n",
    "\n",
    "\\begin{align}  P(Y=1|do(X=1)) &= P((1 \\wedge q)\\vee n_y) \\\\\n",
    "                              &= P((1 \\wedge n_q)\\vee n_y) \\;\\;\\; \\text{(since $q=n_q$)} \\\\\n",
    "                              &= P(n_q \\vee n_y) \\\\   \n",
    "                              &= P(n_q)+P(n_y)-P(n_q \\wedge n_q) \\\\ \n",
    "                              &= 0.9+0.2-0.9*0.2 \\\\ \n",
    "                              &= 0.92 \\end{align}\n",
    "                      \n",
    "And\n",
    "\\begin{align}  P(Y=1) = 0.56 \\;\\;\\; \\text{(from 1.1.1.)}\\end{align}\n",
    "\n",
    "And finally,\n",
    "\\begin{align}  P(Y=0,X=0) &= P((0 \\wedge q)\\vee n_y=0) \\\\\n",
    "                          &= P(0 \\vee n_y =0) \\\\   \n",
    "                          &= P(n_y =0)\\\\ \n",
    "                          &= 0.8 \\end{align}\n",
    "                              \n",
    "Therefore, we have\n",
    "\\begin{align} PS &= \\frac{P(Y=1|do(X=1))-P(Y=1)}{P(Y=0,X=0)} \\\\\n",
    "                 &= \\frac{0.92-0.56}{0.8} \\\\\n",
    "                 &= 0.45 \\end{align}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Probability of Neccessity and Sufficiency, and Identifiability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find $PNS$ for this problem, we assume we know the structural assignment and $X$ is exogenous."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In problem 1.1. we only assume monotonicity and know the structural assignment of each variable. In this case, we can apply **Theorem 9.2.15** (pg 293) of Causality, to find the $PNS$:\n",
    "\n",
    "\\begin{align} PNS = P(Y=1|do(X=1))-P(Y=1|do(X=0)) \\end{align}\n",
    "\n",
    "In 1.1. we found that  \n",
    "\n",
    "\\begin{align} P(Y=1|do(X=1)) = 0.92 \\end{align}\n",
    "\n",
    "and \n",
    "\n",
    "\\begin{align} P(Y=1|do(X=0)) = 0.2 \\end{align}\n",
    "\n",
    "Therefore, we find \n",
    "\n",
    "\\begin{align} PNS = 0.92-0.2 = 0.722 \\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this problem, we assume $X$ is exogenous and $Y$ is monotonic and we use the conditionals to find $PNS$, $PN$, and $PS$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have\n",
    "\n",
    "\\begin{align} PNS = P(Y=1|X=1)-P(Y=1|X=0) = 0.9198813 - 0.1992071 = 0.7206742 \\end{align}\n",
    "\n",
    "and \n",
    "\n",
    "\\begin{align} PN = \\frac{PNS}{P(Y=1|X=1)}= \\frac{0.7206742}{0.9198813} = 0.7834426 \\end{align}\n",
    "\n",
    "and finally,\n",
    "\n",
    "\\begin{align} PS = \\frac{PNS}{P(Y=0|X=0)} = \\frac{PNS}{1-P(Y=1|X=0)} = \\frac{0.7206742}{1-0.1992071} = 0.89995 \\end{align}\n",
    "                 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Mediation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because noise variables for T, E, and C are all 0 during our scenerio, we can write our model as\n",
    "\n",
    "\\begin{cases}\n",
    "  X = N_x \\\\\n",
    "  T = 3*X \\\\\n",
    "  E = 2*T + 8*X \\\\\n",
    "  Y = I(E > 10)\n",
    "\\end{cases}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total effect can be calculated as:\n",
    "\n",
    "\\begin{align}  TE_{Y} &= E(Y|do(X=1)) - E(Y|do(x=0)) \\\\\n",
    "                  &= E(I((2(3(1)) + 8(1)) > 10)) - E(I((2(3(0)) + 8(0)) > 10)) \\\\\n",
    "                  &= E(I(14 > 10)) - E(I(0 > 10)) \\\\\n",
    "                  &= E(1) - E(0) \\\\\n",
    "                  &= 1 \\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NIE can be calculated as:\n",
    "    \\begin{align}  NIE_{Y} &= E(Y|do(X=0),T=E(T|X=1)) - E(Y|do(X=0),T=E(T|X=0)) \\\\\n",
    "                      &= E(I(2(3) > 10)) - E(I(0 > 10)) \\\\\n",
    "                      &= E(I(6 > 10)) - E(I(0 > 10)) \\\\\n",
    "                      &= E(0 - 0) \\\\\n",
    "                      &= 0\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CDE with T=0 can be calculated as:\n",
    "    \\begin{align}  CDE_{Y} &= E(Y|do(X=1),do(T=0)) - E(Y|do(X=0),do(T=0)) \\\\\n",
    "                      &= E(I(2(0) + 8(1)) > 10)) - E(I(2(0) + 8(0) > 10)) \\\\\n",
    "                      &= E(I(8 > 10)) - E(I(0 > 10)) \\\\\n",
    "                      &= E(0 - 0) \\\\\n",
    "                      &= 0\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reverse NIE can be calculated as:\n",
    "    \\begin{align}  NIE_{RY} &= E(Y|do(X=0),T=E(T|X=1)) - E(Y|do(X=1),T=E(T|X=1)) \\\\\n",
    "                      &= E(I(2(3) > 10)) - E(I(2(3) + 8(1) > 10)) \\\\\n",
    "                      &= E(I(6 > 10)) - E(I(14 > 10)) \\\\\n",
    "                      &= E(0 - 1) \\\\\n",
    "                      &= -1\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NDE can be calculated as:\n",
    "\\begin{align}\n",
    "TE &= NDE - rev NIE \\\\\n",
    "1 &= NDE - (-1) \\\\\n",
    "NDE &= 0\n",
    "\\end{align}\n",
    "\n",
    "Given that the calculated NDE is 0, the implication is that the majority of the total effect from the feature is derived from indirect effects. This suggests that the feature X has a limited direct effect on conversion of a user, and that the effects of X are largely mediated through T and E."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having noise variables with variance set to 0 allows us to reason about expected values while largely having probabilities of expectations be equal to either 0 or 1. Variance in noise creates probabilities outside of 0 or 1, which then changes the expectation of values. Say Y = I(8 + n > 10) where n is noise that follows a normal distribution. In our case with 0 variance on noise, this would evaluate to 0 100% of the time, making the E(Y) = 0. However, depending on the variance of the noise, the expression would sometimes evaluate to 1 if there was enough positive noise, and the resulting E(y) would fall somewhere above 0.\n",
    "\n",
    "In our case, TE = 1 and revNIE = -1. However, because revNIE was driven off E(I(14>10)), if we add noise to this equation:\n",
    "E(I(14 + n >10))\n",
    "it no longer evaluates to 1. The resulting revNIE would be some value less than 1, meaning NDE would be greater than 0 and would appear to have a greater effect on the overall results. Depending on how variant the noise is, it would be possible to find a result where NDE makes up closer to half the total effect, which would obviously result in a much different conclusion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The addition of the deep neural network g weakens the analysis techniques above and makes it challenging to draw conclusions from that analysis. Neural networks are typically supervised and use inductive reasoning techniques and curve fitting rather than deductive or causal techniques. This means that the values coming out of g which are driving E(Y) in the above analysis are not going to be reliable in the context of causal model reasoning as they are going to be driven off of fitting to training data rather than percieved causal relationships.\n",
    "\n",
    "The neural network acts no more than a highly nonlinear approximator, only taking into account correlations, and with no regard to the causal dependancies. Since $U$ and $X$ are independent, but conditionally dependent when conditioned on $Y$, the neural network will most likely pick up on this false signal and relate some of the effect of $U$ to $X$. Therefore, changing the values of the total effect and natural direct effect (increasing the magnitude of both). \n",
    "\n",
    "Additionally, even if this were not the case, given the opacity that comes with using deep neural networks, its unclear if it would even be possible to perform the above analysis if we wanted to. Because we are looking at differences in expectation, this would require us to know the expectation curve of g for the values being used in it, which is unlikely given that this is a classic shortcoming of neural netorks.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Effect of the treatment on the treated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "data = np.genfromtxt('.\\HW4.csv', delimiter=',', skip_header =1, names = ['nan','x','y','z'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) \n",
    "First, we show how to obtain $E(Y_{X=0}|X=1)$\n",
    "\n",
    "Since\n",
    "\\begin{align}  P(Y_{X=0}|X=1) &= \\sum_{z}P(Y_{X=0},Z=z|X=1) \\\\\n",
    "                              &= \\sum_{z}P(Y_{X=0}|Z=z,X=1)P(Z=z|X=1) \\;\\;\\; \\text{(marginalize over $z$)}\\\\\n",
    "                              &= \\sum_{z}P(Y_{X=0}|Z=z,X=0)P(Z=z|X=1) \\;\\;\\; \\text{(since $X \\perp Y_X | Z$)}\\\\\n",
    "                              &= \\sum_{z}P(Y|Z=z,X=0)P(Z=z|X=1) \\;\\;\\; \\text{(by Consistency Axiom)}\\end{align}\n",
    "we have \n",
    "\\begin{align}  E(Y_{X=0}|X=1)= \\sum_{z}E(Y|Z=z,X=0)P(Z=z|X=1) \\end{align}\n",
    "\n",
    "Since $Y$ is a continuous random variable, we find that \n",
    "\\begin{align} E(Y_{X=0}|X=1) = \\sum_{z} \\Big[\\Big(\\frac{1}{|i: z_i=z, x_i=0|} \\sum_{i: z_i=z, x_i=0} y_i \\Big)                        \\Big(\\frac{1}{|i: x_i=1|} \\sum_{i: z_i=z,x_i=1} z_i \\Big)\\Big] \\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) We find $E(Y_{X=1}|X=1)$. \n",
    "\n",
    "By the **Consistency Axiom** (Corollary 7.3.2., Causality, pg 229) we have \n",
    "\n",
    "\\begin{align} P(Y_{X=1}|X=1) = P(Y|X=1)\\end{align}\n",
    "\n",
    "Thus, \n",
    "\n",
    "\\begin{align} E(Y_{X=1}|X=1) = E(Y|X=1)=\\frac{1}{|i: x_i=1|} \\sum_{i: x_i=1} y_i\\end{align}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) We find $E(Y_{X=1}-Y_{X=0}|X=1)$. This is the $ETT$.\n",
    "\n",
    "\\begin{align} P(Y_{X=1}-Y_{X=0}|X=1) = P(Y|X=1)-\\sum_{z}P(Y|Z=z,X=0)P(Z=z|X=1)\\end{align}\n",
    "\n",
    "Thus, \n",
    "\n",
    "\\begin{align} E(Y_{X=1}-Y_{X=0}|X=1) &= E(Y_{X=1}|X=1) - E(Y_{X=0}|X=1) \\\\\n",
    "                                     &= \\frac{1}{|j: x_j=1|} \\sum_{j: x_j=} \\big[y_j\\big] - \\sum_{z} \\Big[\\Big(\\frac{1}{|i: z_i=z, x_i=0|} \\sum_{i: z_i=z, x_i=0} y_i \\Big) \\Big(\\frac{1}{|i: x_i=1|} \\sum_{i: z_i =z, x_i=1} z_i \\Big)\\Big] \\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) We find $E(Y_{X=1}-Y_{X=0})$. This is the Control Direct Effect ($CDE$)\n",
    "\n",
    "Since $Z$ satisfies the backdoor criterion, we find\n",
    "\n",
    "\\begin{align} P(Y_{X=1})=\\sum_{z}P(Y|X=1,Z=z)P(Z=z) \\end{align}\n",
    "and\n",
    "\\begin{align} P(Y_{X=0})=\\sum_{z}P(Y|X=0,Z=z)P(Z=z) \\end{align}\n",
    "Therefore, \n",
    "\\begin{align} P(Y_{X=1}-Y_{X=0})=\\sum_{z}\\lbrack P(Y|X=1,Z=z)-P(Y|X=0,Z=z) \\rbrack P(Z=z) \\end{align}\n",
    "\n",
    "Thus, \n",
    "\n",
    "\\begin{align} E(Y_{X=1}-Y_{X=0}) &=\\sum_{z}\\lbrack E(Y|X=1,Z=z)-E(Y|X=0,Z=z) \\rbrack P(Z=z) \\\\ \n",
    "                                 &=\\sum_{z} \\bigg[\\Big(\\frac{1}{|i: z_i=z, x_i=1|} \\sum_{i: z_i=z, x_i=1} y_i \\Big) -\\Big(\\frac{1}{|i: z_i=z, x_i=0|} \\sum_{i: z_i=z, x_i=0} y_i \\Big) \\bigg]\\frac{1}{\\text{num obs}} \\sum_{i: z_i=z} z_i \\end{align}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate these from data we first write some helper functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exp_Y_given_Z_X(z,x):\n",
    "    y_vals =  [data['y'][i] for i, (x_val, z_val) in enumerate(zip(data['x'], data['z']))\n",
    "               if x_val == x and z_val == z ]\n",
    "    return sum(y_vals)/len(y_vals)\n",
    "\n",
    "def prob_Z_given_X(z,x):\n",
    "    z_vals =  [data['z'][i] for i, x_val in enumerate(data['x']) if x_val == x]\n",
    "    return z_vals.count(z) / len(z_vals)\n",
    "    \n",
    "def exp_Y_given_X(x):\n",
    "    y_vals =  [data['y'][i] for i, x_val in enumerate(data['x']) if x_val == x]\n",
    "    return sum(y_vals)/len(y_vals)\n",
    "\n",
    "def prob_Z(z):\n",
    "    z_vals =  list(data['z'])\n",
    "    return z_vals.count(z) / len(z_vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we calculate expectation of $ETT$: $E(Y_{X=1}-Y_{X=0}|X=1)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3456080617640547\n"
     ]
    }
   ],
   "source": [
    "ETT = exp_Y_given_X(x=1) - sum([exp_Y_given_Z_X(z=z_val, x=0)*prob_Z_given_X(z=z_val, x=1) for z_val in [0,1]])\n",
    "print(ETT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the expectation of the $CDE$: $E(Y_{X=1}-Y_{X=0})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2144011179770868\n"
     ]
    }
   ],
   "source": [
    "CDE = sum([ (exp_Y_given_Z_X(z=z_val, x=1) - exp_Y_given_Z_X(z=z_val, x=0))*prob_Z(z=z_val) for z_val in [0,1]])\n",
    "print(CDE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding paragraph explaining results .......\n",
    "..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
